{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cc3a05-6a5f-4d2f-a2c1-cef095e74824",
   "metadata": {},
   "source": [
    "# Implementing and using iterable datasets: What Could Go Wrong?\n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "```python\n",
    "for batch in DataLoader(dataset, batch_size=..., num_workers=...):\n",
    "    # Training here\n",
    "```\n",
    "\n",
    "\n",
    "``dataset`` <-- This is what we'll talk about.\n",
    "There are 2 types of datasets:\n",
    "\n",
    "- Indexable, \"MapStyle\"\n",
    "- Iterable (glorified Python generator). Potentially more powerful, but also much more dangerous\n",
    "\n",
    "\n",
    "There are good reasons to use Iterable datasets (streaming, packing data into chunks to save io/bandwith latency, etc.).\n",
    "And there are good reasons to avoid them.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Understand different issues *users* (you ðŸ«µ) have to deal with when using **iterable** datasets.\n",
    "\n",
    "**Disclaimer**: this talk might be confusing. It's actually the point (kinda).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8149cc9-8a81-4707-a731-95fdce554f20",
   "metadata": {},
   "source": [
    "## Let's start with the basics\n",
    "\n",
    "#### Map-style datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a62715-0c6a-4e21-a2c8-201620fcc9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
      "tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n",
      "tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "tensor([60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
      "tensor([70, 71, 72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87, 88, 89])\n",
      "tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class MyMapStyleDS:\n",
    "    \n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        \n",
    "    def __getitem__(self, idx):  # Returns the i'th sample\n",
    "        # Here: read from disk [+ decoding] [+ transforms]\n",
    "        sample = idx\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    \n",
    "mapstyle_ds = MyMapStyleDS()\n",
    "mapstyle_dl = data.DataLoader(mapstyle_ds, batch_size=10)\n",
    "\n",
    "for batch in mapstyle_dl:\n",
    "    print(batch)\n",
    "    # Here: forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3f0b5-a652-4717-86d1-e48d05303026",
   "metadata": {},
   "source": [
    "#### Iterable datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0046826-0e29-43da-b03e-f1a13e8df510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
      "tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n",
      "tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "tensor([60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
      "tensor([70, 71, 72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87, 88, 89])\n",
      "tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n"
     ]
    }
   ],
   "source": [
    "class MyIterableDS(data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        \n",
    "    def __iter__(self):  # iterate over samples\n",
    "        # Here: read from disk [+ decoding] [+ transforms]\n",
    "        for sample in range(self.size):\n",
    "            yield sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    \n",
    "iter_ds = MyIterableDS()\n",
    "iter_dl = data.DataLoader(iter_ds, batch_size=10)\n",
    "\n",
    "for batch in iter_dl:\n",
    "    print(batch)\n",
    "    # Here: forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a4c2e3-433d-44a7-a788-1715fb59e3cc",
   "metadata": {},
   "source": [
    "### So far so good\n",
    "## Let's add some parallelism  ðŸ•ºðŸ’ƒ\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "- DataLoader parallelism\n",
    "- DDP parallelism (if we have time, which we won't)\n",
    "\n",
    "Fun fact: they're not mutually exclusive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3725d1f-7253-4f8a-96de-a3cc75bb01bf",
   "metadata": {},
   "source": [
    "### DataLoader parallelism\n",
    "\n",
    "#### Map-style - EZPZ lemon squeezy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f268c08-5f42-4f93-9612-e17f6c58520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
      "tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n",
      "tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "tensor([60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
      "tensor([70, 71, 72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87, 88, 89])\n",
      "tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n"
     ]
    }
   ],
   "source": [
    "mapstyle_dl = data.DataLoader(mapstyle_ds, batch_size=10, num_workers=4)\n",
    "\n",
    "for batch in mapstyle_dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190df695-40d0-4391-90ae-40002b85cb9a",
   "metadata": {},
   "source": [
    "#### Iterable - ~EZPZ lemon squeezy~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac29b80-cc17-494f-88c7-1d50c1abab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
      "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
      "tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "tensor([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n",
      "tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n",
      "tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n",
      "tensor([30, 31, 32, 33, 34, 35, 36, 37, 38, 39])\n",
      "tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n",
      "tensor([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "tensor([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "tensor([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "tensor([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])\n",
      "tensor([60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
      "tensor([60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
      "tensor([60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
      "tensor([60, 61, 62, 63, 64, 65, 66, 67, 68, 69])\n",
      "tensor([70, 71, 72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([70, 71, 72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([70, 71, 72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([70, 71, 72, 73, 74, 75, 76, 77, 78, 79])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87, 88, 89])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87, 88, 89])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87, 88, 89])\n",
      "tensor([80, 81, 82, 83, 84, 85, 86, 87, 88, 89])\n",
      "tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n",
      "tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n",
      "tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n",
      "tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n"
     ]
    }
   ],
   "source": [
    "iter_dl = data.DataLoader(iter_ds, batch_size=10, num_workers=4)\n",
    "\n",
    "for batch in iter_dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f21f8f-9dea-488c-8257-89271936115b",
   "metadata": {},
   "source": [
    "### Oops. What went wrong?\n",
    "\n",
    "\n",
    "Let's dive into the [DataLoader internals](./imgs/from_mapstyle_to_iterable.html).\n",
    "\n",
    "\n",
    "Mapstyle\n",
    "\n",
    "<img src=\"imgs/mapstyle.png\" width=\"500\"/>\n",
    "\n",
    "Iterable\n",
    "\n",
    "<img src=\"imgs/iterable.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/sharding.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Recap\n",
    "\n",
    "- **Map-style dataset**: main DataLoader process is able to request specific indices from each worker\n",
    "- **Iterable dataset**: there's no notion of \"indices\". All the DataLoader can do is to request the \"next\" sampler from each worker, via `None`.\n",
    "  - So we need to tell each worker which samples belong to them.\n",
    "  - We have to do that **manually**.\n",
    "  - There's no standard or cannonical way.\n",
    "  \n",
    " \n",
    "TL;DR: it's hard ðŸ¥²\n",
    "\n",
    "<img src=\"imgs/iterable_with_sharding.png\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36611ae0-34c2-4802-a1a7-b74eb3f6d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36])\n",
      "tensor([ 1,  5,  9, 13, 17, 21, 25, 29, 33, 37])\n",
      "tensor([ 2,  6, 10, 14, 18, 22, 26, 30, 34, 38])\n",
      "tensor([ 3,  7, 11, 15, 19, 23, 27, 31, 35, 39])\n",
      "tensor([40, 44, 48, 52, 56, 60, 64, 68, 72, 76])\n",
      "tensor([41, 45, 49, 53, 57, 61, 65, 69, 73, 77])\n",
      "tensor([42, 46, 50, 54, 58, 62, 66, 70, 74, 78])\n",
      "tensor([43, 47, 51, 55, 59, 63, 67, 71, 75, 79])\n",
      "tensor([80, 84, 88, 92, 96])\n",
      "tensor([81, 85, 89, 93, 97])\n",
      "tensor([82, 86, 90, 94, 98])\n",
      "tensor([83, 87, 91, 95, 99])\n"
     ]
    }
   ],
   "source": [
    "class MyIterableDS(data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        \n",
    "    def __iter__(self):  # iterate over samples\n",
    "        worker_info = data.get_worker_info()\n",
    "        num_workers = worker_info.num_workers\n",
    "        worker_id = worker_info.id\n",
    "        \n",
    "        for i, s in enumerate(range(self.size)):\n",
    "            if i % num_workers == worker_id:\n",
    "                yield s\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    \n",
    "iter_ds = MyIterableDS()\n",
    "iter_dl = data.DataLoader(iter_ds, batch_size=10, num_workers=4)\n",
    "\n",
    "for batch in iter_dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae47a3-c386-48c2-9a32-d9e14ea425eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Works OK, but:\n",
    "\n",
    "- Manual and boilerplate code, and there's no standard\n",
    "- Notice the difference with Map-style Dataset (not a big deal tho)\n",
    "- **Notice the batch size at the end!!**. This can have [bad consequences](https://github.com/pytorch/data/issues/302) when batch-norm is involved. Solution: use `drop_last=True`; but ideally we shouldn't need to.\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Now let's take a look at DDP parallelism\n",
    "\n",
    "What is DDP (Distributed Data Parallel)?\n",
    "- N copies of the model, typically on N GPUs (== N DDP processes).\n",
    "- The N models see different parts of the data  <-- **That's the important part**\n",
    "- The N models' weights are kept equal via gradient synchronization\n",
    "\n",
    "\n",
    "Let's look at this outside of this notebook (if we have time): see [this file](https://github.com/NicolasHug/iterable_ds_pres/blob/main/issues_with_ddp.py)\n",
    "\n",
    "\\<insert pain here\\>\n",
    "\n",
    "TL;DR:\n",
    "\n",
    "- The exact same sharding issue happens (but for other reasons)\n",
    "- So we need to shard across **DDP workers** (just like we sharded across DataLoader workers above)\n",
    "- DataLoader multi-processing can be embedded within a DDP multi-process:\n",
    "  - **So we need 2 levels of sharding**: DDP *and* DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4bc436b-c149-4b5e-a44e-b8935162d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterableDS(data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        \n",
    "    def __iter__(self):  # iterate over samples\n",
    "        \n",
    "        worker_info = data.get_worker_info()\n",
    "        num_dl_workers = worker_info.num_workers\n",
    "        dl_worker_id = worker_info.id\n",
    "\n",
    "        num_ddp_workers = dist.get_world_size()\n",
    "        ddp_worker_id = dist.get_rank()\n",
    "        \n",
    "        for i, s in enumerate(range(self.size)):  # We need 2 levels of sharding!!\n",
    "            if i % num_ddp_workers == ddp_worker_id:\n",
    "                if i % num_dl_workers == dl_worker_id:\n",
    "                    yield s\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3f977-c362-4a69-96e8-b8266f982f0d",
   "metadata": {},
   "source": [
    "But IRL you'll need a lot of glue code (e.g. only shard when DDP is on).\n",
    "\n",
    "## More fun: shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e78c35-2300-47e2-aeed-1524dfcdc812",
   "metadata": {},
   "source": [
    "#### Map-style: EZPZ lemon squeezy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59adea2e-f534-4ff0-b650-10dd8ba08a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([67, 35, 27, 93, 50, 68, 33,  7, 19, 51])\n",
      "tensor([37, 26, 87, 82, 60,  0, 62,  1, 69, 42])\n",
      "tensor([ 5, 57, 32, 58, 73, 75, 17, 64, 91, 29])\n",
      "tensor([44, 36, 81, 22, 28, 55, 24, 18, 53, 77])\n",
      "tensor([48, 61, 56, 74, 52, 83, 21, 89,  6, 20])\n",
      "tensor([54, 80, 92, 40, 72, 16, 65, 79, 31,  3])\n",
      "tensor([14, 63, 13, 95, 46, 94,  2, 88, 41, 45])\n",
      "tensor([70, 11, 25, 85, 47,  9, 84, 15, 90, 66])\n",
      "tensor([43, 78, 71, 34, 30, 86, 96, 12,  4, 59])\n",
      "tensor([97, 98, 10, 99, 76,  8, 49, 39, 23, 38])\n"
     ]
    }
   ],
   "source": [
    "sampler = data.RandomSampler(mapstyle_ds)\n",
    "mapstyle_dl = data.DataLoader(mapstyle_ds, batch_size=10, num_workers=4, sampler=sampler)\n",
    "\n",
    "for batch in mapstyle_dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0160985-14d6-4f10-8c09-5149be154929",
   "metadata": {},
   "source": [
    "#### Iterable: ~EZPZ lemon squeezy~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab4bfc8-2fe5-4741-b841-bc99292a515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = data.RandomSampler(iter_ds)\n",
    "iter_ds = MyIterableDS()\n",
    "# iter_dl = data.DataLoader(iter_ds, batch_size=10, num_workers=4, sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d38589-d91c-4b0b-b294-574e240595d8",
   "metadata": {},
   "source": [
    "#### OK, let's shuffle manually then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36d75617-35c9-46c3-a221-4315360c4e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 20,  0, 76, 56, 40, 92, 96, 60, 44])\n",
      "tensor([73, 53, 33, 69, 41,  9, 77, 97, 37, 93])\n",
      "tensor([62,  2, 46, 34, 50, 66, 82, 86, 74, 26])\n",
      "tensor([67, 51, 59, 27, 71, 47, 35, 91, 55, 95])\n",
      "tensor([52, 68,  8, 80, 12, 48, 32, 24, 88, 36])\n",
      "tensor([85, 13, 45, 57, 21, 89,  5, 61, 49, 29])\n",
      "tensor([22, 58, 94, 14, 54, 18, 42, 10, 70, 78])\n",
      "tensor([ 7, 75, 11, 63, 87, 43, 23, 83, 31, 15])\n",
      "tensor([84, 64, 72, 16, 28])\n",
      "tensor([81, 65,  1, 25, 17])\n",
      "tensor([90, 30, 98,  6, 38])\n",
      "tensor([99, 79,  3, 39, 19])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class MyIterableDS(data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        \n",
    "    def __iter__(self):  # iterate over samples\n",
    "        worker_info = data.get_worker_info()\n",
    "        num_workers = worker_info.num_workers\n",
    "        worker_id = worker_info.id\n",
    "        \n",
    "        buffer = []\n",
    "        \n",
    "        for i, s in enumerate(range(self.size)):\n",
    "            if i % num_workers == worker_id:\n",
    "                buffer.append(s)\n",
    "        \n",
    "        random.shuffle(buffer)\n",
    "        \n",
    "        yield from buffer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "iter_ds = MyIterableDS()\n",
    "iter_dl = data.DataLoader(iter_ds, batch_size=10, num_workers=4)\n",
    "\n",
    "for batch in iter_dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53efd5f-0940-4987-8115-3aaaf37b6380",
   "metadata": {},
   "source": [
    "#### Looks random ðŸ¤©\n",
    "\n",
    "Narrator: *It's not*\n",
    "\n",
    "And it's **not obvious** to diagnose. Each individual worker is only shuffling **within its own shard**!\n",
    "\n",
    "<img src=\"imgs/shard_before_shuffle.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "Blue with blue, yellow with yellow... That's not uniform shuffling: the same samples are always being batched together.\n",
    "\n",
    "It can have dramatic effects on the accuracy, especially when the underlying files are stored in a per-class folder structure.\n",
    "\n",
    "## We need to shuffle before we shard\n",
    "\n",
    "\n",
    "So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d3ca0c2-6659-4efc-b72b-6936ab88fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([56,  0, 87,  8, 61, 38,  6,  1, 33, 19])\n",
      "tensor([30, 68, 96, 60, 24, 27, 14, 41, 17, 19])\n",
      "tensor([62, 21, 54, 50, 65,  6,  9,  0, 90, 80])\n",
      "tensor([16, 41, 81, 90, 99, 82, 49, 94,  9, 25])\n",
      "tensor([43, 20, 55, 83, 71, 39, 79, 31, 30, 17])\n",
      "tensor([46, 16, 89, 43, 80, 63, 31, 81, 64, 87])\n",
      "tensor([30, 98, 87, 47, 40, 56, 41, 44, 16,  4])\n",
      "tensor([87, 42, 69,  6, 57, 15, 67, 14, 18, 30])\n",
      "tensor([57, 22, 54, 13, 15])\n",
      "tensor([ 4, 44, 22, 52, 72])\n",
      "tensor([64, 36, 51, 69, 37])\n",
      "tensor([73, 71, 12, 10, 83])\n"
     ]
    }
   ],
   "source": [
    "class MyIterableDS(data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, size=100):\n",
    "        self.size = size\n",
    "        \n",
    "    def __iter__(self):  # iterate over samples\n",
    "        worker_info = data.get_worker_info()\n",
    "        num_workers = worker_info.num_workers\n",
    "        worker_id = worker_info.id\n",
    "        \n",
    "        buffer = []\n",
    "        for s in range(self.size):\n",
    "            buffer.append(s)\n",
    "                \n",
    "        random.shuffle(buffer)  # Shuffle ...\n",
    "        \n",
    "        for i, s in enumerate(buffer):  # ... then shard\n",
    "            if i % num_workers == worker_id:\n",
    "                yield s\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "iter_ds = MyIterableDS()\n",
    "iter_dl = data.DataLoader(iter_ds, batch_size=10, num_workers=4)\n",
    "\n",
    "batches = []\n",
    "for batch in iter_dl:\n",
    "    print(batch)\n",
    "    batches.append(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c251d-7a09-499c-bb6d-4184b435acb4",
   "metadata": {},
   "source": [
    "#### Did it work now? ðŸ¤©\n",
    "\n",
    "\n",
    "Narrator: *it didn't*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "654348e1-f941-4b3e-a962-ae7a721b01ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 1, 2, 3, 1, 2, 1, 1, 1, 2, 2, 3, 2, 1, 2, 1, 1, 2, 1, 1, 1, 4, 2, 1,\n",
       "         1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2,\n",
       "         1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 4, 1, 2, 1, 1, 1, 1]),\n",
       " torch.Size([67]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples = torch.cat(batches)\n",
    "_, counts = torch.unique(all_samples, return_counts=True)\n",
    "counts, counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174cc72-bc98-446e-a631-26bf8902ae56",
   "metadata": {},
   "source": [
    "**Reason**: all workers use a [different RNG seed](https://github.com/pytorch/pytorch/blob/3ac27e78ca5429b47a63826a1bb678031d20bffd/torch/utils/data/_utils/worker.py#L217-L223) for shuffling, so some samples can be missing or duplicated.\n",
    "\n",
    "\n",
    "<img src=\"imgs/iterable_with_seed.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd31be-1f82-4fa7-baf9-d58616d9e786",
   "metadata": {},
   "source": [
    "#### So we need the same RNG seed across workers... Right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a63208-702e-48bb-9670-42b4e28c94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([56,  0, 92, 72, 24, 20, 28, 40, 88, 80])\n",
      "tensor([57,  1, 93, 73, 25, 21, 29, 41, 89, 81])\n",
      "tensor([58,  2, 94, 74, 26, 22, 30, 42, 90, 82])\n",
      "tensor([59,  3, 95, 75, 27, 23, 31, 43, 91, 83])\n",
      "tensor([16,  8, 84, 12, 68, 44, 76, 36, 96, 60])\n",
      "tensor([17,  9, 85, 13, 69, 45, 77, 37, 97, 61])\n",
      "tensor([18, 10, 86, 14, 70, 46, 78, 38, 98, 62])\n",
      "tensor([19, 11, 87, 15, 71, 47, 79, 39, 99, 63])\n",
      "tensor([64, 32,  4, 52, 48])\n",
      "tensor([65, 33,  5, 53, 49])\n",
      "tensor([66, 34,  6, 54, 50])\n",
      "tensor([67, 35,  7, 55, 51])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]),\n",
       " torch.Size([100]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    # This is wrong for *at least* 2 reasons\n",
    "    random.seed(0)  \n",
    "\n",
    "iter_ds = MyIterableDS()\n",
    "iter_dl = data.DataLoader(iter_ds, batch_size=10, num_workers=4, worker_init_fn=worker_init_fn)\n",
    "\n",
    "batches = []\n",
    "for batch in iter_dl:\n",
    "    print(batch)\n",
    "    batches.append(batch)\n",
    "\n",
    "\n",
    "all_samples = torch.cat(batches)\n",
    "_, counts = torch.unique(all_samples, return_counts=True)\n",
    "counts, counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ead0de-a589-4a5d-bccd-d1eaf27a2971",
   "metadata": {},
   "source": [
    "#### Shuffling works now ðŸ¤“ ðŸ¤“ ðŸ¤“ ðŸ¤“\n",
    "### But we broke the random augmentations ðŸ’€ðŸ’€ðŸ’€ðŸ’€\n",
    "### Oh, and no, shuffling still doesn't work: the seed should be epoch-dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c5340-103b-4af7-b38c-20cba9441553",
   "metadata": {},
   "source": [
    "We still want the RNG of the sample transformations (done within the workers) to be different.\n",
    "\n",
    "We **only** want the seed for **shuffling** to be the same across workers (meaning: across DataLoader workers **and** DDP workers).\n",
    "\n",
    "\n",
    "# We need to separate RNG streams\n",
    "## - for shuffling: a unique one across all workers\n",
    "## - for random augmentation: one or more\n",
    "\n",
    "ðŸŽ¶ *Hello Statefulness my old friend...* ðŸŽ¶\n",
    "\n",
    "Idea: pass a shuffling seed in `__init__()` and create a new RNG stream there. But **be careful**: we still need to make sure the seed will be different across epochs (remember [`DistributedSampler.set_epoch()`](https://github.com/pytorch/pytorch/blob/401179f263d5ba22731de107874f41fdd256737f/torch/utils/data/distributed.py#L100)?)\n",
    "\n",
    "\n",
    "And remember: everything is at least twice as hard with multiple GPUs.\n",
    "\n",
    "TL;DR: it's hard ðŸ¥²\n",
    "\n",
    "*The implementation of an iterable dataset that properly handles distributed training and multiprocessing, shuffling and sharding, all within a simple user-friendly API that does not expose low-level implementation details, is left as an exercise to the reader.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3540e-644b-4998-94f2-d9ba00f8f634",
   "metadata": {},
   "source": [
    "## Take away:\n",
    "\n",
    "We've seen a bunch of issues with iterable datasets:\n",
    "  - Related to sharding / multiprocessing\n",
    "    - inside the DataLoader\n",
    "    - outside the DataLoader (DDP)\n",
    "    - when both are involved\n",
    "  - Related to shuffling\n",
    "  - Related to the *order* of these operation: shuffling before sharding\n",
    "\n",
    "Hopefully, you're confused. It is confusing! **You shouldn't be expected to understand all this**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02a2c7-f5af-4610-bc93-ee455b14dd3e",
   "metadata": {},
   "source": [
    "# Don't implement Iterable datasets yourself\n",
    "# Use existing solutions. (But don't assume they're all correct!)\n",
    "# If you're given the power to shuffle and partition manually: be EXTREMELY careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7111f79-e4ae-4e66-9d58-61e159c799d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
